{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularize trajectories\n",
    "def data_prepare(X,Y):\n",
    "    import numpy as np \n",
    "    thr=1e-10\n",
    "    x = np.array(X).reshape(N,i) \n",
    "    x = np.diff(x,axis=1) \n",
    "    sx = np.std(x,axis=1)\n",
    "    x = (x-np.mean(x,axis=1).reshape(len(x),1)) / np.where(sx>thr,sx,1).reshape(len(x),1)   # normalize x data\n",
    "\n",
    "    # regularize labels\n",
    "    label = []\n",
    "    label.append(np.equal(Y,0))\n",
    "    label.append(np.equal(Y,1))\n",
    "    label.append(np.equal(Y,2))\n",
    "    label.append(np.equal(Y,3))\n",
    "    label.append(np.equal(Y,4))\n",
    "    label = np.array(np.transpose(label)) + 0\n",
    "    return(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, None, 250)         255000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                60200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 316,325\n",
      "Trainable params: 316,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "import scipy.io as sci\n",
    "\n",
    "\n",
    "\n",
    "#Building the network\n",
    "model_t2 = Sequential()\n",
    "#first layer: LSTM of dimension 64\n",
    "model_t2.add(LSTM(250,\n",
    "                return_sequences=True,\n",
    "                recurrent_dropout=0.2,\n",
    "                input_shape=(None, 4)\n",
    "                ))\n",
    "\n",
    "\n",
    "model_t2.add(LSTM(50,\n",
    "                    dropout=0,\n",
    "                    recurrent_dropout=0.2,\n",
    "                    ))\n",
    "\n",
    "\n",
    "model_t2.add(Dense(20))\n",
    "\n",
    "\n",
    "#Last layer, fully connected\n",
    "model_t2.add(Dense(5,\n",
    "             activation=\"softmax\",))\n",
    "\n",
    "model_t2.compile(optimizer='adam',\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "#Printing a summary of the built network\n",
    "model_t2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "90000/90000 [==============================] - 1395s 15ms/sample - loss: 0.1997 - categorical_accuracy: 0.9141 - val_loss: 0.1755 - val_categorical_accuracy: 0.9238\n",
      "Epoch 2/5\n",
      "90000/90000 [==============================] - 1347s 15ms/sample - loss: 0.1792 - categorical_accuracy: 0.9227 - val_loss: 0.1836 - val_categorical_accuracy: 0.9182\n",
      "Epoch 3/5\n",
      "90000/90000 [==============================] - 1331s 15ms/sample - loss: 0.1710 - categorical_accuracy: 0.9268 - val_loss: 0.1718 - val_categorical_accuracy: 0.9257\n",
      "Epoch 4/5\n",
      "90000/90000 [==============================] - 1350s 15ms/sample - loss: 0.1629 - categorical_accuracy: 0.9294 - val_loss: 0.1903 - val_categorical_accuracy: 0.9191\n",
      "Epoch 5/5\n",
      "90000/90000 [==============================] - 1341s 15ms/sample - loss: 0.1548 - categorical_accuracy: 0.9331 - val_loss: 0.1946 - val_categorical_accuracy: 0.9183\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "90000/90000 [==============================] - 348s 4ms/sample - loss: 0.1841 - categorical_accuracy: 0.9209 - val_loss: 0.1687 - val_categorical_accuracy: 0.9248\n",
      "Epoch 2/4\n",
      "90000/90000 [==============================] - 351s 4ms/sample - loss: 0.1723 - categorical_accuracy: 0.9259 - val_loss: 0.1686 - val_categorical_accuracy: 0.9279\n",
      "Epoch 3/4\n",
      "90000/90000 [==============================] - 349s 4ms/sample - loss: 0.1659 - categorical_accuracy: 0.9287 - val_loss: 0.1664 - val_categorical_accuracy: 0.9246\n",
      "Epoch 4/4\n",
      "90000/90000 [==============================] - 354s 4ms/sample - loss: 0.1605 - categorical_accuracy: 0.9313 - val_loss: 0.1704 - val_categorical_accuracy: 0.9245\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "90000/90000 [==============================] - 351s 4ms/sample - loss: 0.1791 - categorical_accuracy: 0.9215 - val_loss: 0.1644 - val_categorical_accuracy: 0.9271\n",
      "Epoch 2/4\n",
      "90000/90000 [==============================] - 354s 4ms/sample - loss: 0.1683 - categorical_accuracy: 0.9270 - val_loss: 0.1682 - val_categorical_accuracy: 0.9241\n",
      "Epoch 3/4\n",
      "90000/90000 [==============================] - 352s 4ms/sample - loss: 0.1610 - categorical_accuracy: 0.9296 - val_loss: 0.1699 - val_categorical_accuracy: 0.9255\n",
      "Epoch 4/4\n",
      "90000/90000 [==============================] - 355s 4ms/sample - loss: 0.1556 - categorical_accuracy: 0.9330 - val_loss: 0.1685 - val_categorical_accuracy: 0.9251\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "90000/90000 [==============================] - 350s 4ms/sample - loss: 0.1787 - categorical_accuracy: 0.9223 - val_loss: 0.1839 - val_categorical_accuracy: 0.9245\n",
      "Epoch 2/4\n",
      "90000/90000 [==============================] - 357s 4ms/sample - loss: 0.1681 - categorical_accuracy: 0.9269 - val_loss: 0.1832 - val_categorical_accuracy: 0.9241\n",
      "Epoch 3/4\n",
      "90000/90000 [==============================] - 360s 4ms/sample - loss: 0.1594 - categorical_accuracy: 0.9306 - val_loss: 0.1789 - val_categorical_accuracy: 0.9265\n",
      "Epoch 4/4\n",
      "90000/90000 [==============================] - 355s 4ms/sample - loss: 0.1542 - categorical_accuracy: 0.9330 - val_loss: 0.1817 - val_categorical_accuracy: 0.9234\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "90000/90000 [==============================] - 355s 4ms/sample - loss: 0.1783 - categorical_accuracy: 0.9225 - val_loss: 0.1623 - val_categorical_accuracy: 0.9305\n",
      "Epoch 2/4\n",
      "90000/90000 [==============================] - 354s 4ms/sample - loss: 0.1668 - categorical_accuracy: 0.9281 - val_loss: 0.1646 - val_categorical_accuracy: 0.9283\n",
      "Epoch 3/4\n",
      "90000/90000 [==============================] - 353s 4ms/sample - loss: 0.1600 - categorical_accuracy: 0.9307 - val_loss: 0.1678 - val_categorical_accuracy: 0.9281\n",
      "Epoch 4/4\n",
      "90000/90000 [==============================] - 356s 4ms/sample - loss: 0.1543 - categorical_accuracy: 0.9333 - val_loss: 0.1704 - val_categorical_accuracy: 0.9272\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1701 - categorical_accuracy: 0.9255 - val_loss: 0.1640 - val_categorical_accuracy: 0.9277\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1634 - categorical_accuracy: 0.9284 - val_loss: 0.1638 - val_categorical_accuracy: 0.9300\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1593 - categorical_accuracy: 0.9307 - val_loss: 0.1622 - val_categorical_accuracy: 0.9299\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1693 - categorical_accuracy: 0.9258 - val_loss: 0.1602 - val_categorical_accuracy: 0.9300\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1651 - categorical_accuracy: 0.9281 - val_loss: 0.1629 - val_categorical_accuracy: 0.9277\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1611 - categorical_accuracy: 0.9290 - val_loss: 0.1594 - val_categorical_accuracy: 0.9303\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1677 - categorical_accuracy: 0.9276 - val_loss: 0.1594 - val_categorical_accuracy: 0.9290\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1633 - categorical_accuracy: 0.9288 - val_loss: 0.1614 - val_categorical_accuracy: 0.9287\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 107s 1ms/sample - loss: 0.1593 - categorical_accuracy: 0.9304 - val_loss: 0.1608 - val_categorical_accuracy: 0.9274\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1651 - categorical_accuracy: 0.9273 - val_loss: 0.1617 - val_categorical_accuracy: 0.9277\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1599 - categorical_accuracy: 0.9305 - val_loss: 0.1645 - val_categorical_accuracy: 0.9284\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1571 - categorical_accuracy: 0.9314 - val_loss: 0.1590 - val_categorical_accuracy: 0.9298\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1651 - categorical_accuracy: 0.9281 - val_loss: 0.1610 - val_categorical_accuracy: 0.9284\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 111s 1ms/sample - loss: 0.1612 - categorical_accuracy: 0.9299 - val_loss: 0.1607 - val_categorical_accuracy: 0.9282\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1558 - categorical_accuracy: 0.9315 - val_loss: 0.1633 - val_categorical_accuracy: 0.9278\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      " 2048/90000 [..............................] - ETA: 1:09:51"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[250,250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_2/lstm_4/while/body/_1/strided_slice_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_534874]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9bf96e6c5be4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                 \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                                 \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                                 )\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[250,250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_2/lstm_4/while/body/_1/strided_slice_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_534874]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "i = 825\n",
    "N = 100000\n",
    "batch_sizes = [32, 128, 512, 2048]\n",
    "dataset_used = [1, 4, 5, 20]\n",
    "number_epochs = [5, 4, 3, 2]\n",
    "n = 0\n",
    "for batch in range(len(batch_sizes)):    \n",
    "    for repeat in range(dataset_used[batch]):\n",
    "        data = sci.loadmat('task2_1d' + str(i) + '_' + str(n) + '.mat')\n",
    "        n += 1\n",
    "        X = data['X'][0][0]\n",
    "        Y = data['Y'][0][0].reshape(N,)\n",
    "        x, label = data_prepare(X,Y)\n",
    "        history_t2 = model_t2.fit(x.reshape(len(x),int(i/4),4), \n",
    "                                label, \n",
    "                                epochs=number_epochs[batch], \n",
    "                                batch_size=batch_sizes[batch],\n",
    "                                validation_split=0.1,\n",
    "                                shuffle=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 112s 1ms/sample - loss: 0.1510 - categorical_accuracy: 0.9345 - val_loss: 0.1548 - val_categorical_accuracy: 0.9319\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1434 - categorical_accuracy: 0.9381 - val_loss: 0.1569 - val_categorical_accuracy: 0.9342\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1613 - categorical_accuracy: 0.9300 - val_loss: 0.1608 - val_categorical_accuracy: 0.9296\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 108s 1ms/sample - loss: 0.1544 - categorical_accuracy: 0.9334 - val_loss: 0.1618 - val_categorical_accuracy: 0.9286\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1546 - categorical_accuracy: 0.9321 - val_loss: 0.1551 - val_categorical_accuracy: 0.9341\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1478 - categorical_accuracy: 0.9356 - val_loss: 0.1577 - val_categorical_accuracy: 0.9318\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1527 - categorical_accuracy: 0.9334 - val_loss: 0.1677 - val_categorical_accuracy: 0.9306\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1457 - categorical_accuracy: 0.9364 - val_loss: 0.1735 - val_categorical_accuracy: 0.9297\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1501 - categorical_accuracy: 0.9352 - val_loss: 0.1603 - val_categorical_accuracy: 0.9313\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1432 - categorical_accuracy: 0.9382 - val_loss: 0.1601 - val_categorical_accuracy: 0.9305\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 111s 1ms/sample - loss: 0.1618 - categorical_accuracy: 0.9292 - val_loss: 0.1614 - val_categorical_accuracy: 0.9317\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1531 - categorical_accuracy: 0.9335 - val_loss: 0.1622 - val_categorical_accuracy: 0.9317\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1631 - categorical_accuracy: 0.9290 - val_loss: 0.1591 - val_categorical_accuracy: 0.9304\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1562 - categorical_accuracy: 0.9323 - val_loss: 0.1580 - val_categorical_accuracy: 0.9308\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1617 - categorical_accuracy: 0.9301 - val_loss: 0.1604 - val_categorical_accuracy: 0.9308\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1561 - categorical_accuracy: 0.9318 - val_loss: 0.1621 - val_categorical_accuracy: 0.9296\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1593 - categorical_accuracy: 0.9307 - val_loss: 0.1571 - val_categorical_accuracy: 0.9301\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1541 - categorical_accuracy: 0.9325 - val_loss: 0.1590 - val_categorical_accuracy: 0.9296\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 111s 1ms/sample - loss: 0.1590 - categorical_accuracy: 0.9310 - val_loss: 0.1615 - val_categorical_accuracy: 0.9294\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1525 - categorical_accuracy: 0.9330 - val_loss: 0.1616 - val_categorical_accuracy: 0.9289\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 134s 1ms/sample - loss: 0.1662 - categorical_accuracy: 0.9292 - val_loss: 0.1563 - val_categorical_accuracy: 0.9313\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 107s 1ms/sample - loss: 0.1592 - categorical_accuracy: 0.9316 - val_loss: 0.1544 - val_categorical_accuracy: 0.9328\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 145s 2ms/sample - loss: 0.1641 - categorical_accuracy: 0.9281 - val_loss: 0.1518 - val_categorical_accuracy: 0.9343\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 105s 1ms/sample - loss: 0.1589 - categorical_accuracy: 0.9305 - val_loss: 0.1497 - val_categorical_accuracy: 0.9369\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 105s 1ms/sample - loss: 0.1651 - categorical_accuracy: 0.9284 - val_loss: 0.1522 - val_categorical_accuracy: 0.9345\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1597 - categorical_accuracy: 0.9304 - val_loss: 0.1531 - val_categorical_accuracy: 0.9331\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1638 - categorical_accuracy: 0.9298 - val_loss: 0.1547 - val_categorical_accuracy: 0.9311\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1577 - categorical_accuracy: 0.9330 - val_loss: 0.1540 - val_categorical_accuracy: 0.9324\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1643 - categorical_accuracy: 0.9283 - val_loss: 0.1591 - val_categorical_accuracy: 0.9328\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1579 - categorical_accuracy: 0.9308 - val_loss: 0.1611 - val_categorical_accuracy: 0.9293\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1623 - categorical_accuracy: 0.9294 - val_loss: 0.1514 - val_categorical_accuracy: 0.9345\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1564 - categorical_accuracy: 0.9316 - val_loss: 0.1536 - val_categorical_accuracy: 0.9332\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1654 - categorical_accuracy: 0.9281 - val_loss: 0.1582 - val_categorical_accuracy: 0.9325\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1589 - categorical_accuracy: 0.9316 - val_loss: 0.1567 - val_categorical_accuracy: 0.9327\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1615 - categorical_accuracy: 0.9295 - val_loss: 0.1532 - val_categorical_accuracy: 0.9328\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 111s 1ms/sample - loss: 0.1569 - categorical_accuracy: 0.9322 - val_loss: 0.1522 - val_categorical_accuracy: 0.9339\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 109s 1ms/sample - loss: 0.1637 - categorical_accuracy: 0.9288 - val_loss: 0.1595 - val_categorical_accuracy: 0.9271\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1580 - categorical_accuracy: 0.9320 - val_loss: 0.1619 - val_categorical_accuracy: 0.9277\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 110s 1ms/sample - loss: 0.1629 - categorical_accuracy: 0.9293 - val_loss: 0.1592 - val_categorical_accuracy: 0.9311\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 112s 1ms/sample - loss: 0.1583 - categorical_accuracy: 0.9308 - val_loss: 0.1627 - val_categorical_accuracy: 0.9308\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [512,]\n",
    "dataset_used = [20,]\n",
    "number_epochs = [2,]\n",
    "n = 0\n",
    "for batch in range(len(batch_sizes)):    \n",
    "    for repeat in range(dataset_used[batch]):\n",
    "        data = sci.loadmat('task2_1d' + str(i) + '_' + str(n) + '.mat')\n",
    "        n += 1\n",
    "        X = data['X'][0][0]\n",
    "        Y = data['Y'][0][0].reshape(N,)\n",
    "        x, label = data_prepare(X,Y)\n",
    "        history_t2 = model_t2.fit(x.reshape(len(x),int(i/4),4), \n",
    "                                label, \n",
    "                                epochs=number_epochs[batch], \n",
    "                                batch_size=batch_sizes[batch],\n",
    "                                validation_split=0.1,\n",
    "                                shuffle=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t2.save('Task2_1D_recdout_' + str(i) +'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
