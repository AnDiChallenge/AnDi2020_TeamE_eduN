{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import andi\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import losses, metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from data_split import data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "trajs_from_files = csv.reader(open('sub_more_traj/trajs_post2.txt','r'), delimiter=';', \n",
    "                                        lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "validation = [[],[],[]]\n",
    "for trajs in enumerate(trajs_from_files):\n",
    "    validation[int(trajs[1][0])-1].append(trajs[1][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the networks, two (three) for inference of exponents and switching times, one for classification of first model, one for classification of second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_switch_a_t_new = load_model('nets/task3/1d/task3_new.h5')\n",
    "\n",
    "model_switch_a_t_diff = load_model('nets/task3/1d/diff_task3.h5')\n",
    "\n",
    "model_classi_first = load_model('nets/task3/1d/taks_3_classify_frst_checkout200.h5')\n",
    "\n",
    "model_classi_sec = load_model('nets/task3/1d/taks_3_classify_second_checkout200.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personalized arctan to convert predictions into switching times stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_atan(x1,x2):\n",
    "    y=np.arctan2(x1,x2)\n",
    "    b=y<0\n",
    "    c=b.astype(int)*(2*np.pi)\n",
    "    d=y+c \n",
    "    return    d;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative version, it performed better. For both inference and model detection it is obtained by applying the 1d classifier and averging all dimension independently, we know this may not work well for LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=200\n",
    "for dim in [2]:\n",
    "    pred_hd_a_t=np.zeros((len(validation[1]),3))\n",
    "    \n",
    "    pred_hd_m1_first=np.zeros((len(validation[1]),5))\n",
    "    \n",
    "    pred_hd_m2_sec=np.zeros((len(validation[1]),5))\n",
    "    \n",
    "    valid_ch_dim=np.asarray(validation[dim-1])    #validation in the chosen dimension\n",
    "    for jj in range(dim):\n",
    "        x=valid_ch_dim[:,i*jj:i*(jj+1)]\n",
    "        test_tim_step=np.arange(i)\n",
    "        show_time_coll=np.tile(test_tim_step,(len(x),1))\n",
    "        data_show,label_show,traj_show,times_show=data_split(x,\n",
    "                                                             show_time_coll,\n",
    "                                                                 labels=np.ones(len(x)),\n",
    "                                                                 start_row=0,num_row=len(x),\n",
    "                                                                 traj_len=x.shape[1],n_in=0,n_samples=1,\n",
    "                                                                 p_p=1,hmin=0,hmax=2,limith=False,normalization=True)\n",
    "\n",
    "\n",
    "        pred_a_t_new=model_switch_a_t_new(data_show)\n",
    "        pr_t_new = my_atan(pred_a_t_new[:,2],pred_a_t_new[:,3])*200/(2*np.pi)\n",
    "        pred_hd_a_t[:,:2]+=pred_a_t_new[:,:2]/dim\n",
    "        pred_hd_a_t[:,2]+=pr_t_new/dim\n",
    "        \n",
    "        #model classification uses increments\n",
    "\n",
    "        \n",
    "        data_show_diff,label_show,traj_show_diff,times_show=data_split(np.diff(x,axis=1),\n",
    "                                                             show_time_coll,\n",
    "                                                                 labels=np.ones(len(x)),\n",
    "                                                                 start_row=0,num_row=len(x),\n",
    "                                                                 traj_len=x.shape[1]-1,n_in=0,n_samples=1,\n",
    "                                                                 p_p=1,hmin=0,hmax=2,limith=False,normalization=True)\n",
    "\n",
    "        #first model\n",
    "        pred_m1_first=model_classi_first.predict(data_show_diff)\n",
    "        pred_hd_m1_first+=pred_m1_first/dim\n",
    "        \n",
    "        \n",
    "        #second model\n",
    "\n",
    "        pred_m2_sec=model_classi_sec.predict(data_show_diff)\n",
    "        pred_hd_m2_sec+=pred_m2_sec/dim\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembling the total prediction array  in the format required by submission, giving (t,model 1,a1,model2,a2). For inference, only model new is used and its predictions are averaged in the different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2d=np.zeros((len(validation[1]),5))\n",
    "predictions2d[:,0]=pred_hd_a_t[:,2] # t switch\n",
    "predictions2d[:,1]=np.argmax(pred_hd_m1_first,axis=1)\n",
    "predictions2d[:,2]=pred_hd_a_t[:,0]  # a1\n",
    "predictions2d[:,3]=np.argmax(pred_hd_m2_sec,axis=1)\n",
    "predictions2d[:,4]=pred_hd_a_t[:,1]  #a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions/task3/EXTRAtask3_predictions2d.npy',predictions2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_txt2d = 2*np.ones((len(predictions2d), 6))\n",
    "for i in range(len(predictions2d)):\n",
    "    for j in range(5):\n",
    "        \n",
    "        pred_to_txt2d[i, j+1] = predictions2d[i][j]\n",
    "pred_to_txt = pred_to_txt2d\n",
    "np.savetxt('predictions/task3/EXTRAtask3_predictions2d.txt', pred_to_txt.astype(float), fmt = '%1.5f', delimiter = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the net trained on 2d for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_t2d = tf.keras.models.load_model('nets/task3/2d/taks_3_2d_inference_200.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare_task3_inf(X,dim):\n",
    "    import numpy as np \n",
    "    thr=1e-10\n",
    "    X=np.asarray(X)\n",
    "    print(X.shape)\n",
    "    N=len(X)\n",
    "    #compute length of each trajectory\n",
    "    trj_len=int(X.shape[1]/dim)\n",
    "    xvec=np.zeros((N,trj_len-1,dim+1))\n",
    "    for d in range(dim):\n",
    "        x = np.array(X[:,d*trj_len:(d+1)*trj_len])\n",
    "        x = np.diff(x,axis=1) \n",
    "        sx = np.std(x,axis=1)\n",
    "        xvec[:,:,d] = (x-np.mean(x,axis=1).reshape(len(x),1)) / np.where(sx>thr,sx,1).reshape(len(x),1)   # normalize x data\n",
    "    xvec[:,:,dim]=np.arange(trj_len-1)/trj_len\n",
    "    # regularize labels\n",
    "    \n",
    "    return xvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2340, 400)\n"
     ]
    }
   ],
   "source": [
    "x2d = data_prepare_task3_inf(validation[1],dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2d_u2d = model_a_t2d.predict(x2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_t_2d_u2d =  my_atan(pred2d_u2d[:,2],pred2d_u2d[:,3])*200/(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2d_alt=np.zeros((len(validation[1]),5))\n",
    "predictions2d_alt[:,0]=pr_t_2d_u2d # t switch\n",
    "predictions2d_alt[:,1]=np.argmax(pred_hd_m1_first,axis=1)\n",
    "predictions2d_alt[:,2]=pred2d_u2d[:,0]  # a1\n",
    "predictions2d_alt[:,3]=np.argmax(pred_hd_m2_sec,axis=1)\n",
    "predictions2d_alt[:,4]=pred2d_u2d[:,1]  #a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.4372442 ,   4.        ,   0.74335992,   1.        ,\n",
       "          0.50158453],\n",
       "       [139.06960135,   3.        ,   1.63019156,   3.        ,\n",
       "          1.27498281],\n",
       "       [ 85.12016247,   4.        ,   1.58436382,   3.        ,\n",
       "          1.50557339],\n",
       "       ...,\n",
       "       [194.78039979,   2.        ,   0.65542901,   2.        ,\n",
       "          0.61219156],\n",
       "       [193.62742781,   2.        ,   0.73403025,   4.        ,\n",
       "          0.77530199],\n",
       "       [  3.99988684,   2.        ,   0.59379697,   2.        ,\n",
       "          0.66026318]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2d_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions/task3/EXTRAtask3_predictions2dALT.npy',predictions2d_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
